name: "CaffeNet"
input: "data"
input_shape {
  dim: 1
  dim: 3
  dim: 227
  dim: 227
}

input: "rois"
input_shape {
  dim: 1 # to be changed on-the-fly to num ROIs
  dim: 5 # [batch ind, x1, y1, x2, y2] zero-based indexing
}

layer {
  name: "conv1" type: "Convolution" bottom: "data" top: "conv1"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param { num_output: 96 kernel_size: 7 pad: 3 stride: 2
    weight_filler { type: "gaussian" std: 0.01 }
    bias_filler { type: "constant" value: 0 } 
  }
}

layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
}

layer {
  name: "relu1" type: "ReLU" bottom: "bn1" top: "bn1"  relu_param { negative_slope: 0.2 }
}
layer {
  name: "pool1" type: "Pooling" bottom: "bn1" top: "pool1"
  pooling_param { pool: MAX kernel_size: 3 pad: 0 stride: 2 }
}

layer {
  name: "conv2" type: "Convolution" bottom: "pool1" top: "conv2"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param { num_output: 256 kernel_size: 5 pad: 2 stride: 2 
    weight_filler { type: "gaussian" std: 0.01 }
    bias_filler { type: "constant" value: 1 } 
  }
}

layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn2"
}

layer { 
  name: "relu2" type: "ReLU" bottom: "bn2" top: "bn2"  relu_param { negative_slope: 0.2 }
}
layer {
  name: "pool2" type: "Pooling" bottom: "bn2" top: "pool2"
  pooling_param { pool: MAX kernel_size: 3 pad: 0 stride: 2 }
}

layer {
  name: "conv3" type: "Convolution" bottom: "pool2" top: "conv3"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param { num_output: 512 kernel_size: 3 pad: 1 stride: 1 
    weight_filler { type: "gaussian" std: 0.01 }
    bias_filler { type: "constant" value: 0 } 
  }
}

layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "bn3"
}

layer {
  name: "relu3" type: "ReLU" bottom: "bn3" top: "bn3"  relu_param { negative_slope: 0.2 }
}
layer {
  name: "conv4" type: "Convolution" bottom: "bn3" top: "conv4"
  param { lr_mult: 1 decay_mult: 1}
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param { num_output: 512 kernel_size: 3 pad: 1 stride: 1 
    weight_filler { type: "gaussian" std: 0.01 }
    bias_filler { type: "constant" value: 1 } 
  }
}

layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "bn4"
}


layer {
  name: "relu4" type: "ReLU" bottom: "bn4" top: "bn4" relu_param { negative_slope: 0.2 }
}



layer {
  name: "conv5" type: "Convolution" bottom: "bn4" top: "conv5"
  param { lr_mult: 1 decay_mult: 1}
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param { num_output: 512 kernel_size: 3 pad: 1 stride: 1 
    weight_filler { type: "gaussian" std: 0.01 }
    bias_filler { type: "constant" value: 1 } 
  }
}

layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "conv5"
  top: "bn5"
}

layer {
  name: "relu5" type: "ReLU" bottom: "bn5" top: "bn5" relu_param { negative_slope: 0.2 }
}



layer {
  name: "conv6" type: "Convolution" bottom: "bn5" top: "conv6"
  param { lr_mult: 1 decay_mult: 1}
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param { num_output: 128 kernel_size: 3 pad: 1 stride: 1 
    weight_filler { type: "gaussian" std: 0.01 }
    bias_filler { type: "constant" value: 1 } 
  }
}

layer {
  name: "bn6_1"
  type: "BatchNorm"
  bottom: "conv6"
  top: "bn6_1"
}

layer {
  name: "relu5" type: "ReLU" bottom: "bn6_1" top: "bn6_1" relu_param { negative_slope: 0.2 }
}




layer {
  name: "roi_pool5"
  type: "ROIPooling"
  bottom: "bn6_1"
  bottom: "rois"
  top: "pool5"
  roi_pooling_param {
    pooled_w: 6
    pooled_h: 6
    spatial_scale: 0.0625 # 1/16
  }
}

layer {
  name: "fc6" type: "InnerProduct" bottom: "pool5" top: "fc6"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  inner_product_param {
    num_output: 4096
    weight_filler { type: "gaussian" std: 0.005 }
    bias_filler { type: "constant" value: 1 }
  }
}

layer {
  name: "bn6"
  type: "BatchNorm"
  bottom: "fc6"
  top: "bn6"
}


layer { name: "relu6" type: "ReLU" bottom: "bn6" top: "bn6"  relu_param { negative_slope: 0.2 }
}

layer {
  name: "drop6" type: "Dropout" bottom: "bn6" top: "bn6"
  dropout_param { dropout_ratio: 0.5 }
}

layer {
  name: "fc7" type: "InnerProduct" bottom: "bn6" top: "fc7"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  inner_product_param {
    num_output: 4096
    weight_filler { type: "gaussian" std: 0.005 }
    bias_filler { type: "constant" value: 1 } 
  }
}

layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "bn7"
}

layer { name: "relu7" type: "ReLU" bottom: "bn7" top: "bn7"  relu_param { negative_slope: 0.2 }
}

layer {
  name: "drop7" type: "Dropout" bottom: "bn7" top: "bn7"
  dropout_param { dropout_ratio: 0.5 } 
}

layer {
  name: "cls_score" type: "InnerProduct" bottom: "bn7" top:
  "cls_score_0"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  inner_product_param {
    num_output: 20
    weight_filler { type: "gaussian" std: 0.01 }
    bias_filler { type: "constant" value: 0 }
  }
}

layer {
  name: "bbox_pred" type: "InnerProduct" bottom: "bn7" top:
  "bbox_pred_0"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  inner_product_param {
    num_output: 80
    weight_filler { type: "gaussian" std: 0.001 }
    bias_filler { type: "constant" value: 0 }
  }
}


layer { name: "cls_prob" type: "Softmax" bottom: "cls_score_0" top: "cls_prob" }

# layer {
#   name: "sum_loss_cls" type: "SoftmaxWithLoss" bottom: "cls_score" bottom:
#   "labels" top: "sum_loss_cls" loss_weight: 0
# }
# 
# layer {
#   name: "loss_cls" type: "SoftmaxWithLoss" bottom: "cls_score_0" bottom:
#   "labels" top: "loss_cls" loss_weight: 0.5
# }
# 
# layer { 
#   name: "da_loss_cls" type: "SoftmaxWithLoss" bottom: "cls_score_1"
#   bottom: "labels" top: "da_loss_cls" loss_weight: 0.5
# }
# 
# layer {
#   name: "loss_bbox" type: "SmoothL1Loss" bottom: "bbox_pred_0" bottom: "bbox_targets" 
#   bottom: "bbox_loss_weights" top: "loss_bbox" loss_weight: 0.5
# }
# 
# layer {
#   name: "da_loss_bbox" type: "SmoothL1Loss" bottom: "bbox_pred_1" bottom: "bbox_targets" 
#   bottom: "bbox_loss_weights" top: "da_loss_bbox" loss_weight: 0.5
# }
# 
# layer {
#   name: "sum_loss_bbox" type: "SmoothL1Loss" bottom: "bbox_pred" bottom: "bbox_targets" 
#   bottom: "bbox_loss_weights" top: "sum_loss_bbox" loss_weight: 0
# }
